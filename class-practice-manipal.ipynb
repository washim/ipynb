{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m)\n",
       "\u001b[36mdistData\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cmd1.sc:2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val distData = sc.parallelize(data)\n",
    "distData.collect().foreach(println)\n",
    "distData.take(2).foreach(println)  \n",
    "sc.setLogLevel(\"ERROR\")\n",
    "distData.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n",
      "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdistFile\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = wordcount.txt MapPartitionsRDD[4] at textFile at cmd3.sc:1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distFile = sc.textFile(\"wordcount.txt\")\n",
    "distFile.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "76\n",
      "92\n",
      "120\n",
      "99\n",
      "106\n",
      "567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlines\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = wordcount.txt MapPartitionsRDD[6] at textFile at cmd4.sc:1\n",
       "\u001b[36mlineLengths\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = MapPartitionsRDD[7] at map at cmd4.sc:2\n",
       "\u001b[36mtotalLength\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m567\u001b[39m\n",
       "\u001b[36mres4_5\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = MapPartitionsRDD[7] at map at cmd4.sc:2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"wordcount.txt\")\n",
    "val lineLengths = lines.map(s => s.length)\n",
    "lineLengths.collect().foreach(println)\n",
    "val totalLength = lineLengths.reduce((a, b) => a + b)\n",
    "System.out.println(totalLength)\n",
    "lineLengths.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlines\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = wordcount.txt MapPartitionsRDD[9] at textFile at cmd5.sc:1\n",
       "\u001b[36mpairs\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = MapPartitionsRDD[10] at map at cmd5.sc:2\n",
       "\u001b[36mcounts\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = ShuffledRDD[11] at reduceByKey at cmd5.sc:3\n",
       "\u001b[36mres5_3\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\n",
       "    \u001b[32m\"It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages\"\u001b[39m,\n",
       "    \u001b[32m1\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m\"and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\u001b[39m,\n",
       "    \u001b[32m1\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"wordcount.txt\")\n",
    "val pairs = lines.map(s => (s, 1))\n",
    "val counts = pairs.reduceByKey((a, b) => a + b)\n",
    "counts.collect()\n",
    "counts.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n",
      "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s\n",
      "It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages\n",
      "and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtxtFile\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = wordcount.txt MapPartitionsRDD[13] at textFile at cmd6.sc:1\n",
       "\u001b[36mres6_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m6L\u001b[39m\n",
       "\u001b[36mres6_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"\u001b[39m\n",
       "\u001b[36mlinesWithKey\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[14] at filter at cmd6.sc:4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val txtFile = sc.textFile(\"wordcount.txt\")\n",
    "txtFile.count()\n",
    "txtFile.first()\n",
    "val linesWithKey = txtFile.filter(line => line.contains(\"Lorem\"))\n",
    "linesWithKey.foreach(println)\n",
    "linesWithKey.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mnames2\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[15] at parallelize at cmd7.sc:1\n",
       "\u001b[36mres7_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m3L\u001b[39m\n",
       "\u001b[36mres7_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"apple\"\u001b[39m\n",
       "\u001b[36mres7_3\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"apple\"\u001b[39m, \u001b[32m\"beatty\"\u001b[39m)\n",
       "\u001b[36mteams\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[16] at parallelize at cmd7.sc:5\n",
       "\u001b[36mres7_5\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"indians\"\u001b[39m, \u001b[32m\"brewers\"\u001b[39m, \u001b[32m\"white sox\"\u001b[39m)\n",
       "\u001b[36mres7_6\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"cubs\"\u001b[39m, \u001b[32m\"twins\"\u001b[39m, \u001b[32m\"twins\"\u001b[39m)\n",
       "\u001b[36mres7_7\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"bad news bears\"\u001b[39m, \u001b[32m\"twins\"\u001b[39m, \u001b[32m\"indians\"\u001b[39m)\n",
       "\u001b[36mhockeyTeams\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[20] at parallelize at cmd7.sc:9\n",
       "\u001b[36mres7_9\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mLong\u001b[39m] = \u001b[33mMap\u001b[39m(\n",
       "  \u001b[32m\"wild\"\u001b[39m -> \u001b[32m3L\u001b[39m,\n",
       "  \u001b[32m\"oilers\"\u001b[39m -> \u001b[32m1L\u001b[39m,\n",
       "  \u001b[32m\"jets\"\u001b[39m -> \u001b[32m1L\u001b[39m,\n",
       "  \u001b[32m\"red wings\"\u001b[39m -> \u001b[32m1L\u001b[39m,\n",
       "  \u001b[32m\"whalers\"\u001b[39m -> \u001b[32m1L\u001b[39m,\n",
       "  \u001b[32m\"blackhawks\"\u001b[39m -> \u001b[32m1L\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val names2 = sc.parallelize(List(\"apple\", \"beatty\", \"beatrice\"))\n",
    "names2.count()\n",
    "names2.first\n",
    "names2.take(2)\n",
    "val teams = sc.parallelize(List(\"twins\", \"brewers\", \"cubs\", \"white sox\", \"indians\", \"bad news bears\"))\n",
    "teams.takeSample(true, 3)\n",
    "teams.takeSample(true, 3)    \n",
    "teams.takeSample(false, 3) \n",
    "val hockeyTeams = sc.parallelize(List(\"wild\", \"blackhawks\", \"red wings\", \"wild\", \"oilers\", \"whalers\", \"jets\", \"wild\"))\n",
    "hockeyTeams.map(k => (k,1)).countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore L in count that is my mistake, appending special character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+\n",
      "|age|  id|   name|\n",
      "+---+----+-------+\n",
      "| 25|1201|sathish|\n",
      "| 28|1202|krishna|\n",
      "| 39|1203|  amith|\n",
      "| 23|1204|  javed|\n",
      "| 23|1205| prudvi|\n",
      "+---+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|sathish|\n",
      "|krishna|\n",
      "|  amith|\n",
      "|  javed|\n",
      "| prudvi|\n",
      "+-------+\n",
      "\n",
      "+---+----+-------+\n",
      "|age|  id|   name|\n",
      "+---+----+-------+\n",
      "| 25|1201|sathish|\n",
      "| 28|1202|krishna|\n",
      "| 39|1203|  amith|\n",
      "+---+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 28|    1|\n",
      "| 23|    2|\n",
      "| 25|    1|\n",
      "| 39|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msqlcontext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@66ab12af\n",
       "\u001b[36mdfs\u001b[39m: \u001b[32mDataFrame\u001b[39m = [age: string, id: string ... 1 more field]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlcontext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val dfs = sqlcontext.read.json(\"employee.json\")\n",
    "dfs.show()\n",
    "dfs.printSchema()\n",
    "dfs.select(\"name\").show()\n",
    "dfs.filter(dfs(\"age\") > 23).show()\n",
    "dfs.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
